{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8a3be2-089c-4270-af7b-995e8aaa1514",
   "metadata": {},
   "source": [
    "*Copyright 2023 Google LLC.*\n",
    "\n",
    "*Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at*\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "*Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd389f-c647-4689-8bd2-e7ca86c995cc",
   "metadata": {},
   "source": [
    "![unnamed.png](data:image/png;base64,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)\n",
    "\n",
    "# **Phoebe - Vertex AI demo - Lead score case study**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4751e-0eba-4a77-9fc4-c04671326205",
   "metadata": {},
   "source": [
    "# **Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f8b5f-ec05-4616-b88d-594de78cadd5",
   "metadata": {},
   "source": [
    "1. Admin access to a GCP project.\n",
    "2. Some Terminal experience.\n",
    "3. Some Python experience.\n",
    "4. Some experience with Jupyter notebooks.\n",
    "5. Access to Vertex AI Workbench to run this notebook. See [here](https://cloud.google.com/vertex-ai-notebooks) to learn more about Vertex AI Workbench."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660c1f1-0d8c-4a68-b180-9597919c9687",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090e378-81ca-4be3-891e-f040753387b3",
   "metadata": {},
   "source": [
    "Phoebe is a solution that leverages AI/ML models to make predictions based on sGTM data.\n",
    "\n",
    "This notebook is a demo showing how to use Google Cloud Vertex AI to train a model, evaluate it and deploy to an endpoint in a single pipeline.\n",
    "\n",
    "**THE NOTEBOOK IS FOR DEMO PURPOSES ONLY. IT MIGHT NOT REPRESENT ALL THE BEST PRACTICES AND THE CODE FROM HERE SHOULD NOT BE DEPLOYED INTO PRODUCTION.**\n",
    "\n",
    "**YOU TAKE RESPONSIBILITY FOR THE GCP COSTS ASSOCIATED WITH RUNNING THIS NOTEBOOK.**\n",
    "\n",
    "**THIS IS NOT AN OFFICIAL GOOGLE PRODUCT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d76a4e-99dd-45bc-86cb-fcf65cb9503f",
   "metadata": {},
   "source": [
    "# **Case study**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f3804-e78f-414e-bfff-f8861aedcf83",
   "metadata": {},
   "source": [
    "This notebook demos a case study where an advertiser would use sGTM data to predict lead score for lead gen when a user signs up form.\n",
    "\n",
    "The notebook will use syntehtic data generated in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c02599-2294-412e-a220-f465b89e9047",
   "metadata": {},
   "source": [
    "# **Installs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af4bbb-9710-4736-bd5e-c2146ec2d077",
   "metadata": {},
   "source": [
    "The first step is to generate a file with package equirements (it will be used later to create a Docker image) and then install the required packages in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98abe2bb-3a34-45f8-baa6-7e7d1ee5d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "scikit-learn == 1.2.2\n",
    "pandas == 1.5.3\n",
    "numpy == 1.23.5\n",
    "tensorflow == 2.13.0\n",
    "google-cloud-aiplatform[prediction] >= 1.16.0\n",
    "google-cloud-bigquery == 3.10.0\n",
    "google-cloud-bigquery-connection == 1.12.1\n",
    "google-cloud-bigquery-storage == 2.22.0\n",
    "google-cloud-core == 2.3.3\n",
    "google-cloud-functions == 1.13.2\n",
    "google-cloud-storage == 2.8.0\n",
    "grpcio-status == 1.48.2\n",
    "db-dtypes == 1.1.1\n",
    "fsspec == 2023.6.0\n",
    "gcsfs == 2023.6.0\n",
    "kfp == 2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7366eb-5340-4f20-a12c-2070dbaee119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: pandas==1.5.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: numpy==1.23.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: tensorflow==2.13.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-bigquery==3.10.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-connection==1.12.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.12.1)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage==2.22.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-core==2.3.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-functions==1.13.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.13.2)\n",
      "Requirement already satisfied: google-cloud-storage==2.8.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: grpcio-status==1.48.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.48.2)\n",
      "Requirement already satisfied: db-dtypes==1.1.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: fsspec==2023.6.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2023.6.0)\n",
      "Requirement already satisfied: gcsfs==2023.6.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2023.6.0)\n",
      "Requirement already satisfied: kfp==2.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 1)) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (24.3.7)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (1.62.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0->-r requirements.txt (line 4)) (0.34.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (1.34.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (1.23.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-connection==1.12.1->-r requirements.txt (line 7)) (0.12.7)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-core==2.3.3->-r requirements.txt (line 9)) (2.28.2)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.5.5 in /opt/conda/lib/python3.10/site-packages (from grpcio-status==1.48.2->-r requirements.txt (line 12)) (1.63.0)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from db-dtypes==1.1.1->-r requirements.txt (line 13)) (9.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.6.0->-r requirements.txt (line 15)) (3.9.3)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.6.0->-r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.6.0->-r requirements.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (0.16)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (26.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.0.1->-r requirements.txt (line 16)) (1.26.18)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform>=1.16.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform>=1.16.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (2.0.3)\n",
      "Requirement already satisfied: docker>=5.0.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (7.0.0)\n",
      "Requirement already satisfied: fastapi<0.103.1,>=0.71.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.103.0)\n",
      "Requirement already satisfied: httpx<0.25.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.24.1)\n",
      "Requirement already satisfied: starlette>=0.17.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.27.0)\n",
      "Requirement already satisfied: uvicorn>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2023.6.0->-r requirements.txt (line 15)) (4.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.13.0->-r requirements.txt (line 4)) (0.42.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.103.1,>=0.71.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.10.14)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core==2.3.3->-r requirements.txt (line 9)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core==2.3.3->-r requirements.txt (line 9)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core==2.3.3->-r requirements.txt (line 9)) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.25.0,>=0.23.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from httpx<0.25.0,>=0.23.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.17.3)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.25.0,>=0.23.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.25.0,>=0.23.0->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.0.1->-r requirements.txt (line 16)) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.0.1->-r requirements.txt (line 16)) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.10.0->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette>=0.17.1->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (3.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 4)) (2.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.16.0->uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.16.0; extra == \"prediction\"->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (12.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.17.1->google-cloud-aiplatform[prediction]>=1.16.0->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-core==2.3.3->-r requirements.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.0.1->-r requirements.txt (line 16)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d09c5-57a1-4a9f-89b2-7926958d8a49",
   "metadata": {},
   "source": [
    "**Remember to restart your runtime (kernel) after the installation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5974e4-38d9-4423-8661-27db90df4d08",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b3bc1-d99a-4ff9-ab45-4b35a5eabfef",
   "metadata": {},
   "source": [
    "Next step is to import all the Python libraries required to run all the cells in this notebook. You shouldn't need to import anything later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1569c036-518d-48e4-8c24-e2c41e8dbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 08:29:09.720251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import Any, Final, Mapping, NamedTuple, Sequence\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud.aiplatform.prediction import local_model\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "from google.cloud.bigquery import enums\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49dea4d-ca18-41dd-8c71-3c922cd8c903",
   "metadata": {},
   "source": [
    "# **[YOUR INPUT/ACTION NEEDED] - GCP SETUP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870a7e3-0eaf-4be1-8a08-7e7edac4aaf2",
   "metadata": {},
   "source": [
    "**NO CODE, OTHER THAN IN THIS SECTION, SHOULD BE CHANGED TO RUN THIS NOTBEOOK SUCCESSFULLY.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6a9938-fc78-41a1-a0ca-6ccd7702c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. \"demo_modules\"  (A name for a local directory where demo code chunks are\n",
    "# going to be saved)\n",
    "_DEMO_MODULES_DIRECTORY: Final[str] = \"demo_modules\"\n",
    "\n",
    "# E.g. \"custom_predictor\"  (A name for a local directory where Custom Predictor\n",
    "# artifacts are going to be saved)\n",
    "_CUSTOM_PREDICTOR_DIRECTORY: Final[str] = \"custom_predictor\"\n",
    "\n",
    "# E.g. \"demo-project\" (Your GCP project id)\n",
    "_GCP_PROJECT_ID: Final[str] = \"\"\n",
    "\n",
    "# E.g. \"01234567891\" (Your GCP project number)\n",
    "_GCP_PROJECT_NUMBER: Final[str] = \"\"\n",
    "\n",
    "# E.g. \"europe-west1\" (A region name for your GCP resources, should match the region of the other components)\n",
    "_GCP_REGION: Final[str] = \"\"\n",
    "\n",
    "# E.g. \"demo-bucket\" (A name for your GCS bucket)\n",
    "_GCP_BUCKET_NAME: Final[str] = \"\"\n",
    "\n",
    "# E.g. \"eu\" (A location name of your GCS bucket)\n",
    "_GCP_BUCKET_LOCATION: Final[str] = \"\"\n",
    "\n",
    "# E.g. \"gs://demo-bucket/demo\" (A GCS directory name where all artifacts\n",
    "# are going to be saved)\n",
    "_EXPERIMENT_DIRECTORY: Final[str] = os.path.join(\"gs://\", _GCP_BUCKET_NAME, \"demo\")\n",
    "\n",
    "# E.g. \"demo_dataset\" (A name for your BigQuery dataset)\n",
    "_BIGQUERY_DATASET_NAME: Final[str] = \"demo_dataset\"\n",
    "\n",
    "# E.g. \"demoartifactrepository\" (A name for your Artifact Registry repository)\n",
    "_GCP_ARTIFACT_REPOSITORY: Final[str] = \"demoartifactrepository\"\n",
    "\n",
    "# E.g. \"demopredictor:latest\" (A Docker image name for your Custom Predictor)\n",
    "_GCP_PREDICTOR_IMAGE_NAME: Final[str] = \"demopredictor:latest\"\n",
    "\n",
    "# E.g. \"gs://demo-bucket/demo/pipeline\" (A GCS directory name where all\n",
    "# pipeline artifacts are going to be saved)\n",
    "_GCP_PIPELINE_ROOT: Final[str] = os.path.join(_EXPERIMENT_DIRECTORY, \"pipeline\")\n",
    "\n",
    "# E.g. \"demo-end-to-end\"  (A name for your Vertex AI pipeline)\n",
    "_GCP_PIPELINE_NAME: Final[str] = \"demo-end-to-end\"\n",
    "\n",
    "# E.g. \"demopipeline:latest\". (A Docker image name for your Vertex AI pipeline)\n",
    "_GCP_PIPELINE_IMAGE_NAME: Final[str] = \"demopipeline:latest\"\n",
    "\n",
    "# E.g. \"demo_model\"  (A name for your model in Vertex AI)\n",
    "_GCP_MODEL_DISPLAY_NAME: Final[str] = \"demo_model\"\n",
    "\n",
    "# E.g. \"demo_model_endpoint\"  (A name for your Vertex AI endpoint)\n",
    "_GCP_ENDPOINT_DISPLAY_NAME: Final[str] = \"demo_model_endpoint\"\n",
    "\n",
    "# No input needed here\n",
    "_PIPELINE_DOCKER_IMAGE: Final[str] = (\n",
    "    f\"{_GCP_REGION}-docker.pkg.dev/{_GCP_PROJECT_ID}/{_GCP_ARTIFACT_REPOSITORY}/{_GCP_PIPELINE_IMAGE_NAME}\"\n",
    ")\n",
    "# No input needed here\n",
    "_PREDICTOR_IMAGE: Final[str] = (\n",
    "    f\"{_GCP_REGION}-docker.pkg.dev/{_GCP_PROJECT_ID}/{_GCP_ARTIFACT_REPOSITORY}/{_GCP_PREDICTOR_IMAGE_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5c73-12aa-4de2-beef-af51883a4cc2",
   "metadata": {},
   "source": [
    "Some of the above variables need to become environment variables to run terminal commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e263f77-7433-4d83-891c-63dacb968a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GCP_PROJECT_ID\"] = _GCP_PROJECT_ID\n",
    "os.environ[\"CUSTOM_PREDICTOR_DIRECTORY\"] = _CUSTOM_PREDICTOR_DIRECTORY\n",
    "os.environ[\"GCP_ARTIFACT_REPOSITORY\"] = _GCP_ARTIFACT_REPOSITORY\n",
    "os.environ[\"GCP_REGION\"] = _GCP_REGION\n",
    "os.environ[\"GCP_PROJECT_NUMBER\"] = _GCP_PROJECT_NUMBER\n",
    "os.environ[\"GCP_PIPELINE_IMAGE_NAME\"] = _GCP_PIPELINE_IMAGE_NAME\n",
    "os.environ[\"GCP_BUCKET_NAME\"] = _GCP_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7cd4aa-0598-4c9f-b8ea-a09e2970c478",
   "metadata": {},
   "source": [
    "You need to set the above environment variables in your Terminal separately. Execute the following block, copy the output and execute it in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8b18a4-afc4-4c13-bbdd-fe4494a0e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "export GCP_PROJECT_ID=\"junghan-adh-sub-02\"\n",
      "export CUSTOM_PREDICTOR_DIRECTORY=\"custom_predictor\"\n",
      "export GCP_ARTIFACT_REPOSITORY=\"junghanartifactrepository12\"\n",
      "export GCP_REGION=\"asia-northeast1\"\n",
      "export GCP_PROJECT_NUMBER=\"250263195658\"\n",
      "export GCP_PIPELINE_IMAGE_NAME=\"junghaneclpipeline:latest\"\n",
      "export GCP_BUCKET_NAME=\"junghan-sgtm-ecl-price-12\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "export GCP_PROJECT_ID=\"{_GCP_PROJECT_ID}\"\n",
    "export CUSTOM_PREDICTOR_DIRECTORY=\"{_CUSTOM_PREDICTOR_DIRECTORY}\"\n",
    "export GCP_ARTIFACT_REPOSITORY=\"{_GCP_ARTIFACT_REPOSITORY}\"\n",
    "export GCP_REGION=\"{_GCP_REGION}\"\n",
    "export GCP_PROJECT_NUMBER=\"{_GCP_PROJECT_NUMBER}\"\n",
    "export GCP_PIPELINE_IMAGE_NAME=\"{_GCP_PIPELINE_IMAGE_NAME}\"\n",
    "export GCP_BUCKET_NAME=\"{_GCP_BUCKET_NAME}\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9a3ff-1b97-40fe-93cb-e6bc283d7463",
   "metadata": {},
   "source": [
    "The below commands are required to set up your GCP project, so it can support the below demo.\n",
    "\n",
    "Some of the commands will require you to verify your account by following the displayed prompts.\n",
    "\n",
    "**Run all the the below commands in a Terminal rather than in a notebook.**\n",
    "\n",
    "**The order is important.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab42582-ff98-409a-86a2-3fde5b9fd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988fc33-94cc-4fd1-b24c-a595093804d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth application-default set-quota-project $GCP_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531641f-f53c-48ce-af0d-d966141e0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b1bfd-7a5e-49ea-b303-7e0db2222cee",
   "metadata": {},
   "source": [
    "Now, run the next cell directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb78e4e-1cc2-4123-9553-9745091c524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Operation \"operations/acat.p2-250263195658-202f852f-4a68-475e-8ae2-69751b04b308\" finished successfully.\n",
      "Create request issued for: [junghanartifactrepository12]\n",
      "Waiting for operation [projects/junghan-adh-sub-02/locations/asia-northeast1/operations/af429a87-20bc-4508-9b43-4c399b50e107] to complete...\n",
      "......done.\n",
      "Created repository [junghanartifactrepository12].\n",
      "WARNING: Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"asia-northeast1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: asia-northeast1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n",
      "Creating gs://junghan-sgtm-ecl-price-12/...\n",
      "Updated IAM policy for project [junghan-adh-sub-02].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:junghan-adh-sub-02@appspot.gserviceaccount.com\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform-vm.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.notebookServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/aiplatform.user\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-gae-service.iam.gserviceaccount.com\n",
      "  role: roles/appengine.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/artifactregistry.reader\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-measurement@system.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com\n",
      "  role: roles/bigquerydatatransfer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/binaryauthorization.policyEditor\n",
      "- members:\n",
      "  - serviceAccount:250263195658@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcf-admin-robot.iam.gserviceaccount.com\n",
      "  role: roles/cloudfunctions.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-containerscanning.iam.gserviceaccount.com\n",
      "  role: roles/containerscanning.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-dataform.iam.gserviceaccount.com\n",
      "  role: roles/dataform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:junghan-adh-sub-02@appspot.gserviceaccount.com\n",
      "  role: roles/datastore.user\n",
      "- members:\n",
      "  - group:gps-phoebe-admin@google.com\n",
      "  - serviceAccount:250263195658@cloudservices.gserviceaccount.com\n",
      "  - user:tsymonds@google.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@cloud-filer.iam.gserviceaccount.com\n",
      "  role: roles/file.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@firebase-rules.iam.gserviceaccount.com\n",
      "  role: roles/firebaserules.system\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-firestore.iam.gserviceaccount.com\n",
      "  role: roles/firestore.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-logging.iam.gserviceaccount.com\n",
      "  role: roles/logging.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-osconfig.iam.gserviceaccount.com\n",
      "  role: roles/osconfig.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  - user:yjunghan@google.com\n",
      "  role: roles/owner\n",
      "etag: BwYWlylWirY=\n",
      "version: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [junghan-adh-sub-02].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:junghan-adh-sub-02@appspot.gserviceaccount.com\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform-vm.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.notebookServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/aiplatform.user\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-gae-service.iam.gserviceaccount.com\n",
      "  role: roles/appengine.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/artifactregistry.reader\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-measurement@system.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com\n",
      "  role: roles/bigquerydatatransfer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  role: roles/binaryauthorization.policyEditor\n",
      "- members:\n",
      "  - serviceAccount:250263195658@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcf-admin-robot.iam.gserviceaccount.com\n",
      "  role: roles/cloudfunctions.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-containerscanning.iam.gserviceaccount.com\n",
      "  role: roles/containerscanning.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-dataform.iam.gserviceaccount.com\n",
      "  role: roles/dataform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:junghan-adh-sub-02@appspot.gserviceaccount.com\n",
      "  role: roles/datastore.user\n",
      "- members:\n",
      "  - group:gps-phoebe-admin@google.com\n",
      "  - serviceAccount:250263195658@cloudservices.gserviceaccount.com\n",
      "  - user:tsymonds@google.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@cloud-filer.iam.gserviceaccount.com\n",
      "  role: roles/file.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@firebase-rules.iam.gserviceaccount.com\n",
      "  role: roles/firebaserules.system\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-firestore.iam.gserviceaccount.com\n",
      "  role: roles/firestore.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-logging.iam.gserviceaccount.com\n",
      "  role: roles/logging.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-250263195658@gcp-sa-osconfig.iam.gserviceaccount.com\n",
      "  role: roles/osconfig.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:250263195658-compute@developer.gserviceaccount.com\n",
      "  - user:yjunghan@google.com\n",
      "  role: roles/owner\n",
      "etag: BwYWlymBJgs=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $GCP_PROJECT_ID\n",
    "gcloud services enable compute.googleapis.com \\\n",
    "                       containerregistry.googleapis.com \\\n",
    "                       aiplatform.googleapis.com \\\n",
    "                       cloudbuild.googleapis.com \\\n",
    "                       cloudfunctions.googleapis.com \\\n",
    "                       artifactregistry.googleapis.com \\\n",
    "                       storage-component.googleapis.com\n",
    "gcloud artifacts repositories create $GCP_ARTIFACT_REPOSITORY --repository-format=docker --location=$GCP_REGION\n",
    "gcloud auth configure-docker $GCP_REGION-docker.pkg.dev\n",
    "gcloud storage buckets create gs://$GCP_BUCKET_NAME\n",
    "gsutil iam ch serviceAccount:$GCP_PROJECT_NUMBER-compute@developer.gserviceaccount.com:roles/storage.objectAdmin gs://$GCP_BUCKET_NAME\n",
    "gcloud projects add-iam-policy-binding $GCP_PROJECT_ID --member=serviceAccount:$GCP_PROJECT_NUMBER-compute@developer.gserviceaccount.com --role=roles/aiplatform.user\n",
    "gcloud projects add-iam-policy-binding $GCP_PROJECT_ID --member=serviceAccount:$GCP_PROJECT_NUMBER-compute@developer.gserviceaccount.com --role=roles/artifactregistry.reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73356ce5-5d81-46d1-86fa-49bcf1819f14",
   "metadata": {},
   "source": [
    "# **Mock demo data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08784fce-9e64-4e45-95ca-2be9a74d4861",
   "metadata": {},
   "source": [
    "The below variables are dummy variables used to synthetize mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ce4032-b545-4260-80fd-e0f8f06cb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ADDRESSES: Final[tuple] = (\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\")\n",
    "_JOBS: Final[tuple] = (\"it\", \"finance\", \"automobile\", \"music\", \"art\", \"insurance\")\n",
    "_AGES: Final[tuple] = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21)\n",
    "_NUMBER_OF_OBSERVATIONS: Final[int] = 100000\n",
    "_ALL_COLUMNS = [\n",
    "    \"address\",\n",
    "    \"job\",\n",
    "    \"age\",\n",
    "    \"price\"\n",
    "]\n",
    "_CATEGORICAL_COLUMNS: Final[list] = [\n",
    "    \"address\",\n",
    "    \"job\",    \n",
    "]\n",
    "_NUMERICAL_COLUMNS: Final[list] = [\n",
    "    \"age\",\n",
    "    \"price\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b0bc-e806-45a3-a482-204ba9cffacd",
   "metadata": {},
   "source": [
    "The below cell synthetizes the mock demo data.\n",
    "\n",
    "**THIS DATASET DOES NOT REPRESENT ANY REAL DATA PATTERNS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b49005e-6915-4aff-b087-f9f38047e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_observation(*, seed: int) -> Mapping[str, Any]:\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    address = rng.choice(_ADDRESSES)\n",
    "    job = rng.choice(_JOBS)\n",
    "    age = rng.choice(_AGES)\n",
    "    price = int(random.uniform(1, 50))\n",
    "    \n",
    "    return {\n",
    "        \"address\": address,\n",
    "        \"job\": job,\n",
    "        \"age\": age,\n",
    "        \"price\": price\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7d793b-3c5c-4f5a-ac09-52a5eb661675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_dataset(\n",
    "    *, number_of_observations: int, seeds: Sequence[int] | None = None\n",
    ") -> pd.DataFrame:\n",
    "    if not seeds:\n",
    "        seeds = range(0, number_of_observations)\n",
    "    if len(seeds) != number_of_observations:\n",
    "        raise ValueError(\n",
    "            \"the length of seeds: \"\n",
    "            f\"{number_of_observations} != {len(seeds)}\"\n",
    "        )\n",
    "    return pd.DataFrame.from_records(\n",
    "        [synthetize_observation(seed=seed) for seed in seeds]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "345d10b0-91b5-4c80-8bf1-1d3ee5608e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetize_dataset_variable = synthetize_dataset(\n",
    "    number_of_observations=_NUMBER_OF_OBSERVATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60056854-14cd-44ce-b93d-2cfffda87d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      address        job  age  price\n",
      "0           L      music   11     45\n",
      "1           G      music   16      2\n",
      "2           L    finance    3     17\n",
      "3           L         it    4     49\n",
      "4           K  insurance   19      6\n",
      "...       ...        ...  ...    ...\n",
      "99995       H    finance    7      3\n",
      "99996       C        art    8     38\n",
      "99997       G        art   13     17\n",
      "99998       L      music    2     25\n",
      "99999       G        art    5     36\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(synthetize_dataset_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecf5fa-2701-4acb-8a2b-8598bd1a5363",
   "metadata": {},
   "source": [
    "The dataset needs to be split into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda43542-20e4-43c6-9232-83ef9b5bd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = synthetize_dataset_variable.drop(\"price\", axis=1)\n",
    "dependent_variables = synthetize_dataset_variable[\"price\"]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    independent_variables, dependent_variables, test_size=0.2, random_state=0\n",
    ")\n",
    "train_dataset = pd.concat(\n",
    "    [X_train, pd.DataFrame(y_train, columns=[\"price\"])], axis=1\n",
    ")\n",
    "test_dataset = pd.concat(\n",
    "    [X_test, pd.DataFrame(y_test, columns=[\"price\"])], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07284829-cd98-40be-abf3-ee6d5921ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      address         job  age  price\n",
      "10382       M     finance    5     22\n",
      "73171       F  automobile    8     37\n",
      "30938       J  automobile   10     25\n",
      "99310       G         art   20     36\n",
      "58959       I     finance   11     41\n",
      "...       ...         ...  ...    ...\n",
      "21243       N     finance   20      2\n",
      "45891       I          it    9      2\n",
      "42613       E       music   19     34\n",
      "43567       H         art    8     45\n",
      "68268       L  automobile    8      2\n",
      "\n",
      "[80000 rows x 4 columns]\n",
      "      address         job  age  price\n",
      "3582        C   insurance   20     49\n",
      "60498       A   insurance   13     45\n",
      "53227       J  automobile   12     25\n",
      "21333       C   insurance   19     40\n",
      "3885        C  automobile    6     23\n",
      "...       ...         ...  ...    ...\n",
      "60116       J   insurance   14     29\n",
      "2415        A     finance   16     48\n",
      "43763       A  automobile    7      7\n",
      "71345       B   insurance   19     37\n",
      "77687       G          it    9     25\n",
      "\n",
      "[20000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc164d4f-351e-4eea-a3bf-b2d382a85b64",
   "metadata": {},
   "source": [
    "The notebook demoes the case where the training data from GA4 is stored in BiqQuery. Both the train and the test dataset need to be uploaded to BigQuery.\n",
    "\n",
    "Create a BigQuery dataset where all the mock data will be stored. More details in the \"BigQuery upload\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce63b04f-1af1-4b67-b894-a957922617b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "big_query_dataset_name = \".\".join([_GCP_PROJECT_ID, _BIGQUERY_DATASET_NAME])\n",
    "try:\n",
    "    client.get_dataset(big_query_dataset_name)\n",
    "except:\n",
    "    big_query_dataset = bigquery.Dataset(big_query_dataset_name)\n",
    "    big_query_dataset.location = _GCP_BUCKET_LOCATION\n",
    "    big_query_dataset = client.create_dataset(big_query_dataset, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94e02f1c-ee7c-47b5-beec-e1b7b24bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_bigquery(\n",
    "    *,\n",
    "    dataframe: pd.DataFrame,\n",
    "    table_id: str,\n",
    "    job_configuration: bigquery.LoadJobConfig\n",
    ") -> None:\n",
    "    client = bigquery.Client()\n",
    "    load_job = client.load_table_from_dataframe(\n",
    "        dataframe, table_id, job_config=job_configuration\n",
    "    )\n",
    "    load_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e877eab9-27c1-4a63-b960-d021efedf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_configuration = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"address\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"job\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"age\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"price\", \"INT64\")\n",
    "    ],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "train_table_id = \".\".join(\n",
    "    [_GCP_PROJECT_ID, _BIGQUERY_DATASET_NAME, \"train_dataset\"]\n",
    ")\n",
    "train_csv_filepath = os.path.join(_EXPERIMENT_DIRECTORY, \"train_dataset.csv\")\n",
    "\n",
    "test_table_id = \".\".join(\n",
    "    [_GCP_PROJECT_ID, _BIGQUERY_DATASET_NAME, \"test_dataset\"]\n",
    ")\n",
    "test_csv_filepath = os.path.join(_EXPERIMENT_DIRECTORY, \"test_dataset.csv\")\n",
    "\n",
    "upload_dataframe_to_bigquery(\n",
    "    dataframe=train_dataset,\n",
    "    table_id=train_table_id,\n",
    "    job_configuration=job_configuration\n",
    ")\n",
    "upload_dataframe_to_bigquery(\n",
    "    dataframe=test_dataset,\n",
    "    table_id=test_table_id,\n",
    "    job_configuration=job_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4d2ef-330c-46cf-9c46-c2a6daf491d7",
   "metadata": {},
   "source": [
    "It's also required to grant your service account read access to the created BigQuery dataset. It is required to later run the Vertex AI pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9944be87-1129-4f97-ba24-3b736a0675a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset = client.get_dataset(_BIGQUERY_DATASET_NAME)\n",
    "entries = list(dataset.access_entries)\n",
    "entries.append(\n",
    "    bigquery.AccessEntry(\n",
    "        role=\"READER\",\n",
    "        entity_type=enums.EntityTypes.USER_BY_EMAIL,\n",
    "        entity_id=f\"{_GCP_PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n",
    "    )\n",
    ")\n",
    "dataset.access_entries = entries\n",
    "dataset = client.update_dataset(dataset, [\"access_entries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004a705-6b8f-4b4b-96f4-1bb2fc5043d7",
   "metadata": {},
   "source": [
    "# **Mock Python modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0f401-14ec-470a-adb4-5c6c24f7c896",
   "metadata": {},
   "source": [
    "Typically you would use open-source packages and some custom code to preprocess data, train, evaluate and deploy your AI/ML models.\n",
    "\n",
    "The below code are there to simulate those packages in our case study.\n",
    "\n",
    "**THE BELOW CODE DOES NOT REPRESENT ANY REAL CASE SCENARIO AND/OR BEST PRACTICES. IT ONLY SERVES DEMONSTARTION PURPOSES TO ENABLE DEMO MODEL TRAINING LATER.**\n",
    "\n",
    "First, you need to create a directory where all the modules are going to be set up. This step is unnecessary in a real-life scenario, becuase typically there would be part of installable / importable packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5935e42e-7f43-45bf-83b4-ed2a844fcc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory demo_modules: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir $_DEMO_MODULES_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4018e-9ca4-4868-89ee-77d658f09cfd",
   "metadata": {},
   "source": [
    "In the demo case we're going to create only 2 modules, one for data preprocesing and the other for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c6554c6-bdd8-4bd5-ba8a-1204b79594f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_modules/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $_DEMO_MODULES_DIRECTORY/preprocessing.py\n",
    "import os\n",
    "import pickle\n",
    "from typing import Final\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "_CATEGORICAL_COLUMNS: Final[list] = [\n",
    "    \"address\",\n",
    "    \"job\"\n",
    "]\n",
    "_NUMERICAL_COLUMNS: Final[list] = [\n",
    "    \"age\",\n",
    "    \"price\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_numerical_variables(\n",
    "    *, experiment_directory: str, dataset: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Returns preprocessed numerical variables.\n",
    "\n",
    "  Args:\n",
    "    experiment_directory: The directory where all the training artifacts are\n",
    "      stored.\n",
    "    dataset: The dataset to be preprocessed.\n",
    "  \"\"\"\n",
    "  age_values = dataset[[\"age\"]]\n",
    "  min_max_scaler_path = os.path.join(\n",
    "      experiment_directory, \"min_max_scaler.pickle\"\n",
    "  )\n",
    "  if tf.io.gfile.exists(min_max_scaler_path):\n",
    "    with tf.io.gfile.GFile(min_max_scaler_path, \"rb\") as artifact:\n",
    "      min_max_scaler = pickle.load(artifact)\n",
    "  else:\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    min_max_scaler.fit(age_values)\n",
    "    with tf.io.gfile.GFile(min_max_scaler_path, \"wb\") as artifact:\n",
    "      pickle.dump(min_max_scaler, artifact)\n",
    "  age_price = pd.DataFrame(\n",
    "      min_max_scaler.transform(age_values),\n",
    "      columns=age_values.columns,\n",
    "      index=age_values.index,\n",
    "  )\n",
    "  return pd.concat(\n",
    "      [dataset[_NUMERICAL_COLUMNS].drop(\"age\", axis=1), age_price],\n",
    "      axis=1,\n",
    "  )\n",
    "\n",
    "\n",
    "def preprocess_categorical_variables(\n",
    "    *, experiment_directory: str, dataset: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Returns preprocessed categorical variables.\n",
    "\n",
    "  Args:\n",
    "    experiment_directory: The directory where all the training artifacts are\n",
    "      stored.\n",
    "    dataset: The dataset to be preprocessed.\n",
    "  \"\"\"\n",
    "  categorical_features = dataset[_CATEGORICAL_COLUMNS]\n",
    "  one_hot_encoder_path = os.path.join(\n",
    "      experiment_directory, \"one_hot_encoder.pickle\"\n",
    "  )\n",
    "  if tf.io.gfile.exists(one_hot_encoder_path):\n",
    "    with tf.io.gfile.GFile(one_hot_encoder_path, \"rb\") as artifact:\n",
    "      one_hot_encoder = pickle.load(artifact)\n",
    "  else:\n",
    "    one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "    one_hot_encoder.fit(categorical_features)\n",
    "    with tf.io.gfile.GFile(one_hot_encoder_path, \"wb\") as artifact:\n",
    "      pickle.dump(one_hot_encoder, artifact)\n",
    "  encoded_categorical_variables_values = (\n",
    "      one_hot_encoder.transform(categorical_features).toarray().astype(np.int32)\n",
    "  )\n",
    "  encoded_categorical_variables_names = one_hot_encoder.get_feature_names_out()\n",
    "  return pd.DataFrame(\n",
    "      encoded_categorical_variables_values,\n",
    "      columns=encoded_categorical_variables_names,\n",
    "      index=categorical_features.index,\n",
    "  )\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    *, experiment_directory: str, dataset: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Returns preprocessed dataset.\n",
    "\n",
    "  Args:\n",
    "    experiment_directory: The directory where all the training artifacts are\n",
    "      stored.\n",
    "    dataset: The dataset to be preprocessed.\n",
    "  \"\"\"\n",
    "  all_columns = _NUMERICAL_COLUMNS + _CATEGORICAL_COLUMNS\n",
    "  if dataset.columns.tolist().sort() != all_columns.sort():\n",
    "    raise ValueError(\n",
    "        \"The columns of the dataset must be the same as the columns of the\"\n",
    "        \" _ALL_COLUMNS.\"\n",
    "    )\n",
    "  numerical_variables = preprocess_numerical_variables(\n",
    "      experiment_directory=experiment_directory, dataset=dataset\n",
    "  )\n",
    "  categorical_variables = preprocess_categorical_variables(\n",
    "      experiment_directory=experiment_directory, dataset=dataset\n",
    "  )\n",
    "    \n",
    "  return pd.concat([categorical_variables, numerical_variables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6006674-9107-4748-93fb-e5ca9c349a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_modules/deployment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $_DEMO_MODULES_DIRECTORY/deployment.py\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def create_endpoint(\n",
    "    *, endpoint_name: str, project_id: str, region: str\n",
    ") -> aiplatform.Endpoint:\n",
    "  \"\"\"Creates an endpoint.\n",
    "\n",
    "  Args:\n",
    "    endpoint_name: The name of the endpoint.\n",
    "    project_id: The GCP project ID.\n",
    "    region: The regions of the GCP project.\n",
    "  \"\"\"\n",
    "  endpoints = aiplatform.Endpoint.list(\n",
    "      filter='display_name=\"{}\"'.format(endpoint_name),\n",
    "      order_by=\"create_time desc\",\n",
    "      project=project_id,\n",
    "      location=region,\n",
    "  )\n",
    "  if len(endpoints) > 0:\n",
    "    endpoint = endpoints[0]\n",
    "  else:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=endpoint_name, project=project_id, location=region\n",
    "    )\n",
    "  return endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a0486-bb94-48c1-acda-4a69d7a65597",
   "metadata": {},
   "source": [
    "# **Vertex AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eb026-8d07-48c4-bb2c-6ca07390b078",
   "metadata": {},
   "source": [
    "## Custom Predictor Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d2c07-71d8-45f7-8853-d4239f527e2c",
   "metadata": {},
   "source": [
    "Certain use cases require development of a [Custom Predictor Routine](https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines) to serve predictions with Vertex AI Endpoints.\n",
    "\n",
    "The first step is to create a local directory where code required for the custom predictor will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad26166e-3eab-40a6-b479-b25b05cbcf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $CUSTOM_PREDICTOR_DIRECTORY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78cc7a-4e77-460f-b021-aae58acb830c",
   "metadata": {},
   "source": [
    "Second, you'll need to write a requirements.txt fil, which will be used later to create a Docker image for that predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c55a9a2b-6d75-413c-9fa9-af0ecb5c30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_predictor/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $_CUSTOM_PREDICTOR_DIRECTORY/requirements.txt\n",
    "tensorflow == 2.13.0\n",
    "google-cloud-aiplatform[prediction] >= 1.22.0\n",
    "pandas == 1.5.3\n",
    "numpy >= 1.23.5\n",
    "scikit-learn == 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aa67914-b8cb-4b48-bc9e-47c8aea3f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_predictor/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $_CUSTOM_PREDICTOR_DIRECTORY/predictor.py\n",
    "import os\n",
    "import pickle\n",
    "from typing import Final, Mapping\n",
    "from google.cloud.aiplatform.prediction import predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "_ALL_COLUMNS: Final[list] = [\n",
    "    \"address\",\n",
    "    \"job\",\n",
    "    \"age\"\n",
    "]\n",
    "\n",
    "\n",
    "class DemoCustomPredictor(predictor.Predictor):\n",
    "  \"\"\"A custom predictor for demo purposes.\n",
    "\n",
    "  Attributes:\n",
    "    numerical_columns: A list with numerical columns.\n",
    "    categorical_columns: A list with categorical columns.\n",
    "    model: A model used to make predictions.\n",
    "    min_max_scaler: A scaler used to scale numerical variables.\n",
    "    one_hot_encoder: A one-hot encoder used to encode categorical variables.\n",
    "    label_encoder: A label encoder used to encode labels.\n",
    "    prices: A price of the item, used in prediction post-processing.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    \"\"\"Initializes the DemoCustomPredictor class.\"\"\"\n",
    "    self.numerical_columns = [\"age\"]\n",
    "    self.categorical_columns = [\n",
    "      \"address\",\n",
    "      \"job\"\n",
    "    ]\n",
    "    return\n",
    "\n",
    "  def load(self, artifacts_uri: str) -> None:\n",
    "    \"\"\"Loads the model, min_max_scaler, one_hot_encoder and label_encoder.\n",
    "\n",
    "    Args:\n",
    "      artifacts_uri: The URI of the experiment directory.\n",
    "    \"\"\"\n",
    "    prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "    with tf.io.gfile.GFile(\"min_max_scaler.pickle\", \"rb\") as artifact:\n",
    "      self.min_max_scaler = pickle.load(artifact)\n",
    "    with tf.io.gfile.GFile(\"one_hot_encoder.pickle\", \"rb\") as artifact:\n",
    "      self.one_hot_encoder = pickle.load(artifact)\n",
    "    with tf.io.gfile.GFile(\"model.pickle\", \"rb\") as artifact:\n",
    "      self.model = pickle.load(artifact)\n",
    "\n",
    "  def preprocess(self, prediction_input: Mapping[str, Mapping]) -> pd.DataFrame:\n",
    "    \"\"\"Returns a preprocessed input.\n",
    "\n",
    "    Args:\n",
    "      prediction_input: A dictionary with the input for the prediction.\n",
    "    \"\"\"\n",
    "    input_data = prediction_input[\"instances\"]\n",
    "    if not isinstance(input_data, list):\n",
    "        input_data = list(input_data)\n",
    "    instances = pd.DataFrame.from_records(input_data)\n",
    "    # self.set_prices_property(dataset=instances)\n",
    "    return self.preprocess_instances(dataset=instances)\n",
    "\n",
    "  def predict(self, instances: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns a prediction result.\n",
    "\n",
    "    Args:\n",
    "      instances: A dataframe with the input for the prediction.\n",
    "    \"\"\"\n",
    "    return self.model.predict(instances)\n",
    "\n",
    "  def postprocess(\n",
    "      self, prediction_results: np.ndarray\n",
    "  ) -> Mapping[str, list[str]]:\n",
    "    \"\"\"Returns a postprocessed prediction result.\n",
    "\n",
    "    Args:\n",
    "      prediction_results: A mapping of \"predictions\" to the postprocessed\n",
    "        predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # postprocessed_predictions = self.min_max_scaler.inverse_transform(\n",
    "        # prediction_results\n",
    "    # ).flatten().tolist()\n",
    "\n",
    "    postprocessed_predictions = prediction_results.flatten().tolist()\n",
    "\n",
    "    \n",
    "    final_predictions = []\n",
    "    final_predictions.append(postprocessed_predictions)\n",
    "    return {\"predictions\": final_predictions}\n",
    "\n",
    "  def preprocess_numerical_variables(\n",
    "      self, *, dataset: pd.DataFrame\n",
    "  ) -> pd.DataFrame:\n",
    "    \"\"\"Returns preprocessed numerical variables.\n",
    "\n",
    "    Args:\n",
    "      dataset: A dataframe to preprocess.\n",
    "    \"\"\"\n",
    "    age_values = dataset[[\"age\"]]\n",
    "    scaled_age = pd.DataFrame(\n",
    "        self.min_max_scaler.transform(age_values),\n",
    "        columns=age_values.columns,\n",
    "        index=age_values.index,\n",
    "    )\n",
    "    return pd.concat(\n",
    "        [dataset[self.numerical_columns].drop(\"age\", axis=1), scaled_age],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "  def preprocess_categorical_variables(\n",
    "      self, *, dataset: pd.DataFrame\n",
    "  ) -> pd.DataFrame:\n",
    "    \"\"\"Returns preprocessed categorical variables.\n",
    "\n",
    "    Args:\n",
    "      dataset: A dataframe to preprocess.\n",
    "    \"\"\"\n",
    "    categorical_features = dataset[self.categorical_columns]\n",
    "    encoded_categorical_variables_values = (\n",
    "        self.one_hot_encoder.transform(categorical_features)\n",
    "        .toarray()\n",
    "        .astype(np.int32)\n",
    "    )\n",
    "    encoded_categorical_variables_names = (\n",
    "        self.one_hot_encoder.get_feature_names_out()\n",
    "    )\n",
    "    return pd.DataFrame(\n",
    "        encoded_categorical_variables_values,\n",
    "        columns=encoded_categorical_variables_names,\n",
    "        index=categorical_features.index,\n",
    "    )\n",
    "\n",
    "  def preprocess_instances(self, *, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns preprocessed dataset for prediction.\n",
    "\n",
    "    Args:\n",
    "      dataset: A dataframe to preprocess.\n",
    "    \"\"\"\n",
    "    if dataset.columns.tolist().sort() != _ALL_COLUMNS.sort():\n",
    "      raise ValueError(\n",
    "          \"The columns of the dataset must be the same as the columns of the\"\n",
    "          \" _ALL_COLUMNS.\"\n",
    "      )\n",
    "    numerical_variables = self.preprocess_numerical_variables(dataset=dataset)\n",
    "    categorical_variables = self.preprocess_categorical_variables(\n",
    "        dataset=dataset\n",
    "    )\n",
    "    return pd.concat([categorical_variables, numerical_variables], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9b656-1206-41d7-bd5f-d7ee8c2f96d3",
   "metadata": {},
   "source": [
    "Next, you need to build and push a custom container for the Custom Predictor Routine.\n",
    "\n",
    "**THE BELOW CELL CAN RUN FOR ABOUT 5 MINUTES.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5083ed0a-42f2-4906-9e32-3503ddbc0af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/opt/conda/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "from custom_predictor.predictor import DemoCustomPredictor\n",
    "\n",
    "output_image_uri = f\"{_GCP_REGION}-docker.pkg.dev/{_GCP_PROJECT_ID}/{_GCP_ARTIFACT_REPOSITORY}/{_GCP_PREDICTOR_IMAGE_NAME}\"\n",
    "custom_local_model = local_model.LocalModel.build_cpr_model(\n",
    "    src_dir=_CUSTOM_PREDICTOR_DIRECTORY,\n",
    "    output_image_uri=output_image_uri,\n",
    "    predictor=DemoCustomPredictor,\n",
    "    requirements_path=os.path.join(\n",
    "        _CUSTOM_PREDICTOR_DIRECTORY, \"requirements.txt\"\n",
    "    ),\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "custom_local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cb637-4e13-4702-ba1c-3523820fb3bc",
   "metadata": {},
   "source": [
    "Some Vertex AI pipeline require custom Docker images to run as this is the case in our demo example.\n",
    "\n",
    "To read more about Vertex AI pipelines, click [here](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n",
    "\n",
    "The first thing, you need to do is to write a Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5eb705a4-0fe9-4170-9e11-84b71f46f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "\n",
    "WORKDIR /vertex_ai_demo\n",
    "\n",
    "COPY . .\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d736f-d125-4d1a-a06c-ea274adcb7f3",
   "metadata": {},
   "source": [
    "## **Push the Custom Predictor Docker image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537252a-564a-4c89-9381-65287abbdd6b",
   "metadata": {},
   "source": [
    "Then, run the below commands to build and push the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9539ef2e-a961-4f40-bd03-ae9c58e14b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/2: Building docker image...\n",
      "Step 2/2: Pushing docker image...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo 'Step 1/2: Building docker image...'\n",
    "docker build --tag=$GCP_REGION-docker.pkg.dev/$GCP_PROJECT_ID/$GCP_ARTIFACT_REPOSITORY/$GCP_PIPELINE_IMAGE_NAME . > /dev/null\n",
    "echo 'Step 2/2: Pushing docker image...'\n",
    "docker push $GCP_REGION-docker.pkg.dev/$GCP_PROJECT_ID/$GCP_ARTIFACT_REPOSITORY/$GCP_PIPELINE_IMAGE_NAME > /dev/null\n",
    "echo 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e13dc-83ce-43fd-b0db-3a5399a2d55d",
   "metadata": {},
   "source": [
    "## **End-to-end pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783f14f-67a5-40d2-a539-6a0a8ea5452a",
   "metadata": {},
   "source": [
    "The first component collect the train and test data from a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8542b9f5-884d-4ad0-ae25-b6a407293185",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=_PIPELINE_DOCKER_IMAGE,\n",
    ")\n",
    "def get_data(\n",
    "    *,\n",
    "    gcp_project_id: str,\n",
    "    bigquery_dataset_name: str,\n",
    "    unprocessed_train_dataset: dsl.Output[dsl.Dataset],\n",
    "    unprocessed_test_dataset: dsl.Output[dsl.Dataset],\n",
    ") -> None:\n",
    "  \"\"\"Gets train and test datasets from BigQuery.\n",
    "\n",
    "  Args:\n",
    "    gcp_project_id: The GCP project ID.\n",
    "    bigquery_dataset_name: The name of the BigQuery dataset.\n",
    "  \"\"\"\n",
    "  from google.cloud import bigquery\n",
    "\n",
    "  client = bigquery.Client()\n",
    "  bigquery_dataset_reference = bigquery.DatasetReference(\n",
    "      gcp_project_id, bigquery_dataset_name\n",
    "  )\n",
    "  train_table_reference = bigquery_dataset_reference.table(\"train_dataset\")\n",
    "  train_table = client.get_table(train_table_reference)\n",
    "  train_dataset = client.list_rows(train_table).to_dataframe(\n",
    "      create_bqstorage_client=False\n",
    "  )\n",
    "  test_table_reference = bigquery_dataset_reference.table(\"test_dataset\")\n",
    "  test_table = client.get_table(test_table_reference)\n",
    "  test_dataset = client.list_rows(test_table).to_dataframe(\n",
    "      create_bqstorage_client=False\n",
    "  )\n",
    "  train_dataset.to_csv(unprocessed_train_dataset.path + \".csv\", index=False)\n",
    "  test_dataset.to_csv(unprocessed_test_dataset.path + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe02f-caa7-4ab8-8c6b-696713bc979f",
   "metadata": {},
   "source": [
    "The second component preprocesses the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ea9b8a1-20c2-47f8-9371-69c78ad0a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=_PIPELINE_DOCKER_IMAGE,\n",
    ")\n",
    "def preprocess_data(\n",
    "    *,\n",
    "    experiment_directory: str,\n",
    "    unprocessed_train_dataset: dsl.Input[dsl.Dataset],\n",
    "    unprocessed_test_dataset: dsl.Input[dsl.Dataset],\n",
    "    processed_train_dataset: dsl.Output[dsl.Dataset],\n",
    "    processed_test_dataset: dsl.Output[dsl.Dataset],\n",
    ") -> None:\n",
    "  import pandas as pd\n",
    "  from demo_modules import preprocessing\n",
    "\n",
    "  train_dataset = preprocessing.preprocess_dataset(\n",
    "      experiment_directory=experiment_directory,\n",
    "      dataset=pd.read_csv(unprocessed_train_dataset.path + \".csv\"),\n",
    "  )\n",
    "  test_dataset = preprocessing.preprocess_dataset(\n",
    "      experiment_directory=experiment_directory,\n",
    "      dataset=pd.read_csv(unprocessed_test_dataset.path + \".csv\")\n",
    "  )\n",
    "  train_dataset.to_csv(processed_train_dataset.path + \".csv\", index=False)\n",
    "  test_dataset.to_csv(processed_test_dataset.path + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c48be-5442-429d-971f-c4ceb4c1a8f0",
   "metadata": {},
   "source": [
    "The third component trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdfc6283-4461-47b2-a237-6d13c828bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=_PIPELINE_DOCKER_IMAGE,\n",
    ")\n",
    "def train_model(\n",
    "    *,\n",
    "    processed_train_dataset: dsl.Input[dsl.Dataset],\n",
    "    trained_model: dsl.Output[dsl.Model],\n",
    ") -> None:\n",
    "  import pandas as pd\n",
    "  from sklearn import ensemble\n",
    "  import tensorflow as tf\n",
    "  import pickle\n",
    "\n",
    "  train_dataset = pd.read_csv(processed_train_dataset.path + \".csv\")\n",
    "  X_train = train_dataset.drop(\"price\", axis=1)\n",
    "  y_train = train_dataset[\"price\"]\n",
    "  random_forest = ensemble.RandomForestClassifier(random_state=0).fit(\n",
    "      X_train, y_train\n",
    "  )\n",
    "  with tf.io.gfile.GFile(trained_model.path + \".pickle\", \"wb\") as artifact:\n",
    "    pickle.dump(random_forest, artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f86d67-9a52-40ee-8a8b-896c5895ccf4",
   "metadata": {},
   "source": [
    "The fourth component evaluates the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a788cb-453f-4bb3-a89b-daaed488a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=_PIPELINE_DOCKER_IMAGE,\n",
    ")\n",
    "def evaluate_model(\n",
    "    *,\n",
    "    experiment_directory: str,\n",
    "    processed_test_dataset: dsl.Input[dsl.Dataset],\n",
    "    trained_model: dsl.Input[dsl.Model],\n",
    ") -> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "  import pandas as pd\n",
    "  import tensorflow as tf\n",
    "  import pickle\n",
    "  from sklearn.metrics import accuracy_score\n",
    "\n",
    "  _ACCURACY_SCORE_THRESHOLD = 0.0\n",
    "\n",
    "  test_dataset = pd.read_csv(processed_test_dataset.path + \".csv\")\n",
    "  X_test = test_dataset.drop(\"price\", axis=1)\n",
    "  y_test = test_dataset[\"price\"]\n",
    "  with tf.io.gfile.GFile(trained_model.path + \".pickle\", \"rb\") as artifact:\n",
    "    model = pickle.load(artifact)\n",
    "  model_score = accuracy_score(model.predict(X_test), y_test)\n",
    "  if model_score > _ACCURACY_SCORE_THRESHOLD:\n",
    "    with tf.io.gfile.GFile(\n",
    "        experiment_directory + \"/model.pickle\", \"wb\"\n",
    "    ) as artifact:\n",
    "      pickle.dump(model, artifact)\n",
    "    deploy_to_endpoint = \"true\"\n",
    "  else:\n",
    "    deploy_to_endpoint = \"false\"\n",
    "  return (deploy_to_endpoint,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a56202-badd-4ff8-8fe3-ce1db6a276cc",
   "metadata": {},
   "source": [
    "The final component deploys the model to a Vertex AI endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a583abdc-0b72-4913-8f31-d67c3f2c3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=_PIPELINE_DOCKER_IMAGE,\n",
    ")\n",
    "def deploy_model(\n",
    "    *,\n",
    "    experiment_directory: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    predictor_image_name: str,\n",
    "    display_name: str,\n",
    "    endpoint_name: str,\n",
    "    vertex_endpoint: dsl.Output[dsl.Artifact],\n",
    "    vertex_model: dsl.Output[dsl.Model],\n",
    ") -> None:\n",
    "  from google.cloud import aiplatform\n",
    "  from google.cloud.aiplatform.prediction import local_model\n",
    "  from google.cloud.aiplatform.compat.types import model as gca_model_compat\n",
    "  from demo_modules import deployment\n",
    "\n",
    "  aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "  container_specifications = gca_model_compat.ModelContainerSpec(\n",
    "      image_uri=predictor_image_name,\n",
    "      predict_route=local_model.DEFAULT_PREDICT_ROUTE,\n",
    "      health_route=local_model.DEFAULT_HEALTH_ROUTE,\n",
    "  )\n",
    "  local_model = local_model.LocalModel(\n",
    "      serving_container_spec=container_specifications\n",
    "  )\n",
    "  model_upload = aiplatform.Model.upload(\n",
    "      display_name=display_name,\n",
    "      local_model=local_model,\n",
    "      artifact_uri=experiment_directory,\n",
    "  )\n",
    "  endpoint = deployment.create_endpoint(\n",
    "      endpoint_name=endpoint_name, project_id=project_id, region=region\n",
    "  )\n",
    "  model_deploy = model_upload.deploy(\n",
    "      machine_type=\"n1-standard-4\",\n",
    "      endpoint=endpoint,\n",
    "      traffic_split={\"0\": 100},\n",
    "      deployed_model_display_name=display_name,\n",
    "  )\n",
    "  vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b3500-06ec-4245-87f1-ab6d8ddd7d2c",
   "metadata": {},
   "source": [
    "You need to assemble an end-to-end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72b6d623-82d7-4bd6-a62a-c10378f80e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=_GCP_PIPELINE_ROOT,\n",
    "    name=_GCP_PIPELINE_NAME,\n",
    ")\n",
    "def run_end2end_pipeline(\n",
    "    gcp_project_id: str = _GCP_PROJECT_ID,\n",
    "    bigquery_dataset_name: str = _BIGQUERY_DATASET_NAME,\n",
    "    experiment_directory: str = _EXPERIMENT_DIRECTORY,\n",
    "    gcp_region: str = _GCP_REGION,\n",
    "    predictor_image_name: str = _PREDICTOR_IMAGE,\n",
    "    display_name: str = _GCP_MODEL_DISPLAY_NAME,\n",
    "    endpoint_name: str = _GCP_ENDPOINT_DISPLAY_NAME,\n",
    "):\n",
    "  data_operation = get_data(\n",
    "      gcp_project_id=gcp_project_id, bigquery_dataset_name=bigquery_dataset_name\n",
    "  )\n",
    "  preprocessing_operation = preprocess_data(\n",
    "      experiment_directory=experiment_directory,\n",
    "      unprocessed_train_dataset=data_operation.outputs[\n",
    "          \"unprocessed_train_dataset\"\n",
    "      ],\n",
    "      unprocessed_test_dataset=data_operation.outputs[\n",
    "          \"unprocessed_test_dataset\"\n",
    "      ],\n",
    "  )\n",
    "  training_operation = train_model(\n",
    "      processed_train_dataset=preprocessing_operation.outputs[\n",
    "          \"processed_train_dataset\"\n",
    "      ]\n",
    "  )\n",
    "  evaluation_operation = evaluate_model(\n",
    "      experiment_directory=experiment_directory,\n",
    "      processed_test_dataset=preprocessing_operation.outputs[\n",
    "          \"processed_test_dataset\"\n",
    "      ],\n",
    "      trained_model=training_operation.outputs[\"trained_model\"],\n",
    "  )\n",
    "  with dsl.Condition(\n",
    "      evaluation_operation.outputs[\"deploy\"] == \"true\",\n",
    "      name=\"deploy-model\",\n",
    "  ):\n",
    "    deploy_model_operation = deploy_model(\n",
    "        experiment_directory=experiment_directory,\n",
    "        project_id=gcp_project_id,\n",
    "        region=gcp_region,\n",
    "        predictor_image_name=predictor_image_name,\n",
    "        display_name=display_name,\n",
    "        endpoint_name=endpoint_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215abded-7c2c-4579-8307-890033e786b3",
   "metadata": {},
   "source": [
    "Finally, you need to compile the pipeline and run it for everything to happen automatically.\n",
    "\n",
    "**THE BELOW CELL CAN RUN FOR ABOUT 15-20 MINUTES.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da278134-f5e7-4aa4-9a37-425f5353b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-northeast1/pipelines/runs/junghan-ecl-end-to-end-price-12-20240421090955?project=250263195658\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/250263195658/locations/asia-northeast1/pipelineJobs/junghan-ecl-end-to-end-price-12-20240421090955\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=run_end2end_pipeline,\n",
    "    package_path=\"vertex_ai_end_to_end_demo.json\",\n",
    ")\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"vertex-ai-end-to-end-demo\",\n",
    "    template_path=\"vertex_ai_end_to_end_demo.json\",\n",
    "    enable_caching=False,\n",
    "    location=_GCP_REGION,\n",
    ")\n",
    "start_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-13:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
